import numpy as np
import pandas as pd
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from dotenv import dotenv_values
from feed import search_podcast, get_episodes, get_episode, download_episode, get_podcast_by_feedId
from db import db_init, get_podcast_list, get_episode_list_by_podcast_id, get_segment_list_by_episode_id, insert
rng = np.random.default_rng(seed=19530)
from core import transcribe

from gensim.models import KeyedVectors
from huggingface_hub import hf_hub_download

config = dotenv_values("../../.env")

model = KeyedVectors.load_word2vec_format(
    hf_hub_download(
        repo_id="Word2vec/german_model",
        filename="german.model"), binary=True, unicode_errors="ignore")


##uvicorn service:app --host 0.0.0.0 --port 80 --reload

# Create connection & collections
max_dimension = 300
db_init(max_dimension)

app = FastAPI()

origins = [
    "http://localhost:3001",
    "http://localhost:8501/",
    "http://localhost:8080",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def embed_text(text, model):
    words = text.split()
    # Filter out words not in the vocabulary
    words_in_vocab = [word for word in words if word in model]
    if not words_in_vocab:
        return None  # If no words in vocabulary, return None or handle accordingly
    # Calculate the mean of word vectors
    embedding = sum(model[word] for word in words_in_vocab) / len(words_in_vocab)
    return embedding

@app.get("/")
async def read_root():
    return {"message": "Welcome to the podcast transcription as a service API!"}


@app.get("/v1/podcast/{podcast_name}")
def get_podcasts_by_name(podcast_name: str):
    '''
    Search for podcast with a certain podcast_name.
    '''
    '''
    podcastindex_config = {
        'api_key': config['PODCASTINDEX_API_KEY'],
        'api_secret': config['PODCASTINDEX_API_SECRET']
    }
    index = podcastindex.init(podcastindex_config)
    return index.search(podcast_name, max_results=20)
    '''
    return search_podcast(podcast_name)


@app.get("/v1/podcast/episodes/{podcast_id}")
async def get_episode_list(podcast_id: str,  max_results: int = 100, last_saved_episode : int = 0):
    '''
    get all episodes of a given podcast up to limit of max_result since last_saved_episode
    '''
    return get_episodes(podcast_id, max_results=max_results)


# VectorDB call
# Items found are already transcribed

@app.get("/v1/vdb/transcribe/episode/{episode_id}")
async def transcribe_episode(episode_id: str):
    # Get Episode from Podcastindex
    episode_detail = get_episode(episode_id=episode_id)
    episode = episode_detail['episode']

    podcast_detail = get_podcast_by_feedId(episode['feedId'])
    podcast = podcast_detail['feed']

    # Download Episode
    filename_audio = download_episode(episode_url=episode['enclosureUrl'], episode_id=episode['id'])

    # Transcribe Episode
    (filename_transcript, transcript) = transcribe(episode['id'], filename_audio, diarize=False)

    # Set up DB
    transcript_parsed = transcript['parsed_transcript']

    # test
    # transcript_parsed = [{'text': ' Hallo und herzlich willkommen zur heutigen Sendung. Heute sprechen wir bei neues Modell Fönix, ein Modell des bei uns an der Hochschule Ansparteniertworte und eine Vielversprächende Leistung bietet. Vielen Dank, auch an unsere Sponsoren, ich soll zwei. Ich zählt zwei ist ein schon Wenscher von Audium-Kipp-Gemini, dass die digitale Transformation in der Automobilindustrie vorantreibt.', 'start': 0.009, 'end': 19.155}, {'text': ' Knowledge Science. Der Podcast über künstliche Intelligenz im Allgemeinen und Natural Language Processing im Spezialen. Mittels KI-Bissen entdecken, aufbereiten und nutzbar machen. Das ist die Idee hinter Knowledge Science. Durch Entmüsteifizierung der künstlichen Intelligenz und vielen praktischen Interviews machen wir dieses Thema Wechendlich greifbar. Willkommen zum Podcasts von Siegurt Schacht und Kasten Lankion.', 'start': 23.166, 'end': 51.63}, {'text': ' Hallo Siegurd und herzlich willkommen. Herr Uleg Matthias Uleg unserer Gastreute. Ja, letztendlich mit Verantwortliche oder Hauptverantwortliche für die Erstellung eines neuen Sprachmodells, was wieder für nichts aus der Asche letztendlich auf einmal da war. Aber bevor wir darauf eingehen, was das genau ist, was es kann, Herr Uleg, wenn sie sich vielleicht einmal kurz selbst vorstellen. Sehr gern, Hallo, anziehungen, die zu handeln.', 'start': 55.35, 'end': 82.688}, {'text': ' Mein Name ist Matthias Ullig, ich bin Student des KDT-Studiengangs an der Hochschule Ansbach. Ich bin jetzt im dritten Semester, gerade mit in der Master-Tesis und habe als mit Wochenend Projekt angefangen und dann doch zu einem bisschen mehr ausgewaltet, mich noch ein bisschen mit dem Training von Sprachmodellen beschäftigt und bin dann so bei Fönigs angelangt und es jetzt verfügbar.', 'start': 83.2, 'end': 108.319}, {'text': ' Ich finde es echt super und deswegen freut es mich total, dass es hier auch bereit erklärt haben bei uns im Podcast mitzumachen. Es ist ja sozusagen zwar Regengel im Schuhingang darüber, dass man nebenbei sich noch ein bisschen was anschauen soll, dann ein bisschen fast tun soll und sich einfach dran gewöhnen soll, weil einfach die 1,5 Jahre in der Regel nicht ganz lange in oben.', 'start': 109.07, 'end': 126.817}, {'text': ' Alles zu lernen, aber es ist natürlich mit Werkständigkeit. Und was man noch so alles macht, als Studenten, oft nicht zu einfach. Und da sieht dann natürlich gleich auf den Zug aufgesprungen sind, Daten vorzubereiten, Modelltraining zu machen, in dem Kontext mit den Forschern zusammen und so als schon super. Also gut ab.', 'start': 126.817, 'end': 148.097}, {'text': ' Also wie ist es richtig verstanden, aber wirklich nebenher, so wocht ein Projekt, also jetzt keine Teil einer Prüfungsleistung erst mal, sondern wirklich nebenher, das finde ich wirklich sehr schön. Also eine ganz tolle Leistung. Aber wir haben das gehört, Phoenix. Was genau ist das? Phoenix ist an Spachmodell, das letztendlich als Chatmodell, oder als Assistenzmodell trainiert wurde, was sie ihren auf dem Mistrollmodell, das ja sehr gut performt hat.', 'start': 149.07, 'end': 177.824}, {'text': ' Da mit der Besonderheit, dass ich nicht auf das normale Mistfallmodell zurückgegriffen habe, sondern auch eine adaptierte Version. Das noch einmal mit 65 Milliarden Token, Deutsche Texte weiter trainiert wurde, einfach mit dem Ziel an besseres Deutsches Modell hinzukriegen, als wie es mit dem englischen vielleicht möglich gewesen wäre.', 'start': 178.677, 'end': 201.749}, {'text': ' Man muss ja aushalten, wenn ich sagt, dass immer kann auch ein bisschen in dem Paper, was in den nächsten Tagen sozusagen verfügbar sein wird, auch mal nachlesen. Aber das Haupttema ist ja das viel dem Modelle auch, wenn man chipschipitiere anschaut oder ähnliches, das war die Adi Deutsch können und so weiter, aber sie sind doch mit relativ wenig deutschen Daten trainiert und so richtig. Es haben mal fokussiert, auf die deutsche Sprache oder auf eine weniger vertriebene Sprache sind die Modelle eher nicht.', 'start': 202.875, 'end': 230.469}, {'text': ' Was sind denn? Das wird mich ja direkt mal interessieren. Sie sagen, Sie mit deutschen Daten, primär mit deutschen Daten noch jetzt weiter trainiert. Was für Daten sind das? Wo haben Sie die Herbkommen? Haben Sie die müssten Sie da selbst noch um wie viele Daten erzeugen, ergänzt, oder wie Sie diese Prozess daraus? Die Daten, die die verwendet haben, laut dem hugging-face repository sind alle öffentlich verfügbar. Also ich lees das gerade mal vor, das sind einmal der Wikipedia.', 'start': 233.831, 'end': 259.258}, {'text': ' wo ich von der Auscar-Corpus aus dem Januar 2023 und dann noch auszüge aus dem der Tagesschau so wie es aussieht. Und dass du sammeln gibt dann 560 Milliarden neue Token, die da nochmal nachgeschoben wurden im casual language modelen.', 'start': 259.804, 'end': 279.701}, {'text': ' Okay, so erst mal in dem, in dem, in dem, in dem, in dem, in dem, in dem, in dem, in dem, in dem, aber vollständigen von von von von diesen Texten, aber wenn sie sagen, es ist ein Chatmodell, ist es ja auch ein hat sein gewisses Superbeis, Feintuning auf, auf Instruktionen im Chat, oder? Genau, so, da muss man, vielleicht gucken, die, der URLM, also das checkt der Leien, eV, dahinter, in Zusammenarbeit mit dem hessischen KI-Zentrum.', 'start': 280.043, 'end': 302.056}, {'text': ' Die haben sich verschiedene Modelle vorgenommen, die Lama Modelle, 7, 13 und 70 Milliarden-Parameter und zusätzlich das Mistrollmodelle. Wir haben für alle, die sind Modelle letztendlich ein weiteres casual languagemodeling vorgenommen, also das Vervollständigen der Texte. Und dann auch für alle noch mal Chatmodelle zur Verfügung gestellt. Für nichts passiert dabei auf dem', 'start': 302.619, 'end': 326.357}, {'text': ' Modell ohne Superweist-Find-Tuning, also auf den Mistrollmodell, das letztendlich deutsche Textverfallständigung gelernt hat. Und darauf aufbauen, habe ich dann mit anderen Daten an den Chatmodell draufgesetzt. Interessante ist er vor allem der Budseste Erdatenerstellung. Weil es ist nicht so, dass man einmal sagt, jetzt guckt man mal und zieht er irgendwo von den Fests die Daten. Vielleicht können Sie daran ein bisschen Wasser zu erzählen, weil es war ja noch nicht ganz so trivial. Das kam dann tatsächlich über das Seffio-Modell,', 'start': 327.056, 'end': 356.544}, {'text': ' Was ja letztendlich so gewisse Vorlage war, für das Modell auch, die sich da in ihrem Paper auf den Ultra Chat Datensatz geschützt haben und in Ultra Feedback Datensatz. Also zwei große synthetisch generierte Datensätze, der Ultra Chat im Seine Ursprung von Globe mit über 700.000 Chat Konversationen, den die Runde gebrochen haben auf 200.000 Konversationen, mit dem dann eben das Säfermodell trainiert, wo wir also eins der', 'start': 356.749, 'end': 385.316}, {'text': ' bekanntesten mit Direct Performance Optimization. Leider aber nur der Datensatz auf Englisch, wo ich dann dazu kam, an wege oder ich mir überlegt habe, wie könnten wir jetzt diesen Datendat der Erwohl sehr gut funktioniert für die deutsche Sprache zu nutzen. Und habe da mit den Erfahrungen aus einem Projekt aus dem Studiengang, dass wir im März gefor bis Juli geführt haben, den Ansatz genommen und zu sagen, ich übersetzt die Daten mit einem auch offen verfügbaren Latsch language model.', 'start': 385.811, 'end': 415.418}, {'text': ' in dem Fall dem Hallmarmodell. Okay und und also das heißt ja, melden wir die automatisiert aus, wir wegen wahrscheinlich englischen Datensätzen in Deutschland gemacht. Wenn Sie da, ich weiß mal, das ist klar und möchte das natürlich alles möglichst automatisiert haben, aber man prüft jetzt sicherlich auch mal so Abschnittsweise. Das heißt wie so die Eindruck von der Qualität von den Erzeugen, Daten von den Übersetzungen,', 'start': 415.947, 'end': 438.183}, {'text': ' Also ich habe im Laufe des Experiments mehrere verschiedene Modelle getestet. Es gibt auch eins von Facebook und Facebook wie im T-Modell. Und das befür das andere Projekt genutzt hatten, das schon vergleichsweise gut funktioniert hat. Also da waren hier und da immer noch Probleme da. Aber das Alma Modell ist dann noch mal deutlich besser, was einfach die Text und Sprachqualität angeht.', 'start': 438.78, 'end': 461.51}, {'text': ' in ihrem Paper Messen, die sich auch gegen den GPT-3 übersetzen. Es gibt den Zwischen auch ein neues Modell, in dem sie sich auch mit GPT-4 messen. Laut dem Paper auch sehr erfolgreich. Die Qualität war sogar ganz gut, das einzige, was so ein bisschen ein Manker war, war das die Unterscheidung zwischen Du und Z. Und aus dem englischen Juh ist es nicht immer ganz klar, welche Formen jetzt zu verwenden. Und da lesen sich die Sätze, dann manchmal ein bisschen Holbrich in der Übersetzung. Aber sonst war ich mit der Qualität sehr zufrieden.', 'start': 461.903, 'end': 491.749}, {'text': ' Haben Sie denn bemerkt, dass man direkt eingestiegen, dass diese potenziellen Fehler, die die sich auf der Grunde übersetzung ergeben sich, denn auch in das Modell einschleichen, in den Antworten, die das Modell generiert. Das ist mir in den Antworten bisher tatsächlich noch nicht aufgefallen.', 'start': 492.329, 'end': 507.073}, {'text': ' Was vielleicht an den Testprobenz liegt, die ich verwendet habe, aber ich von dem, was ich bisher aus den übersetzten Daten gesehen habe, was immer so eine gute Mischung als ob sie jetzt du oder sie nutzt und in der Generalisierung ist es mir ist noch nicht aufgefallen, dass da groß. Was ich auch mal sehr sehr wichtiges Thema ist, was wir im Englischenjahren, der vom kaum haben, ist das Jennern, wenn ich hier weibliche Formen', 'start': 507.381, 'end': 536.408}, {'text': ' Da wird sie ausm Engelschen ja kaum ergeben und bei mir übersetzen, weil sie nicht wie gut die Übersetze da funktionieren oder da viel verdreien bringen, ist ihn darum, was aufgefallen, dass das schon bestärken, schwächen schon sich in dem Modell äußern. Das ist ein guter Punkt, auf denen bin ich noch nicht eingegangen. Dadurch, dass es eben nur an Projekt, das was nebenher gelaufen ist, hatte ich jetzt noch nicht die Zeit, da mich groß tiefer mit den Daten auseinanderzusetzen. Es ist aber ein Thema, was in der Forschungsgruppe vom Professor Schacht und der Hochschule weiter', 'start': 536.903, 'end': 566.544}, {'text': ' vorangetrieben werden soll, die Daten noch mal zu überprüfen und zu schauen, was denn da so drin steckt.', 'start': 566.886, 'end': 583.166}, {'text': ' Allumfängt dieses, aber es ist ehrlich gesagt noch nichts aufgefallen, dass irgendwelche Übersetzung zu wedern. Das zeigt nicht zu der eigentlichen, zu einem Generierungsfehler oder zu der Generierungsanomalie kommt. Man muss auch sagen, wir darf nicht vergessen. Die Modelle, das Modelle ist ja jetzt kein von diesen trainiertes Modell, sondern es ist ja ein Vord trainiertes, also wir haben in von diesen Modellen der Stelle als Basis genommen. Und dann ist ja schon das Thema, dass dieses Grundraining der Sprache,', 'start': 583.166, 'end': 611.903}, {'text': ' Ja, eigentlich schon vorhanden ist, und man im Ende wird der vor allem nur noch das Modell stärker prägt durch das Feintionier oder das DPO-Training an der Stelle, ja. Und da ist auch immer eine, euch, der schon wirklich viel, viel, viel Daten, dass wir uns an das Modell an sich so um prägt, dass dann plötzlich es nicht mehr zwischen du und sie unterscheiden kann. Also das würde ich jetzt nicht erwarten, dass es passiert, ja.', 'start': 611.903, 'end': 636.288}, {'text': ' Die Frage, wer halt wie gut sind, die Ausgangsmodelle schon in diesem Innsicht? Das weiß ich nicht aufzuwaschen untersucht wurde, also ich habe es selbst noch nicht überprüft. Ich weiß ich selbst merke nur, wenn ich selbst wenn ich so ein GPT 3 5 4 oder GPT 4 verwende, dass das Modell oft durch einen anderen kommt mit den Geschlechtern.', 'start': 636.323, 'end': 654.343}, {'text': ' Also ich finde ich mal sage hier, rollen zuweisung, du bist eine, das ist ich erstes Tenden und dann vergisst es das, ob wie man bringt, bringt es die Geschichte mal durch nannand, hätte ja sein können, dass das dadurch, das entweder verstärkt wird oder irgendwie ein bisschen aufgehoben wird. Aber bevor wir jetzt mal auf diese Details eingehen, würde mich mal interessieren, sagen ja, das schlägt einige Bekannte Modelle. Was für Benchmarks oder was für Test haben Sie denn da jetzt durchgeführt, wo es jetzt besonders gut ist.', 'start': 654.497, 'end': 679.189}, {'text': ' Genau, das ist die Evaluation von Deutsche Modelle. Deutsche Modellen ist meines Achts, aber ein bisschen schwierig, weil es eben wenig Benchmarks gibt, die Standardisiert sind, wo man jetzt einfach sagen kann, man vergleicht ist. Wir haben uns da gestützt auf auch vom Leien team ein zur Verfügung gestellter Deutsche Benchmark angelehnt an dem MT-Bench den englischen und auszüge aus dem LM-Evaluation-Harnes. Also der Heller-Swerk test', 'start': 679.77, 'end': 709.548}, {'text': ' Arc und MMLU, genau. Und wir haben dann in den MMLM evaluation handels, letztendlich vorgleichen gegen das Chat-Modell von Leyen, das passiert auf dem Mr. Modell, in den drei Benchmark sind wir besser. Ich glaube immer so um die 4 bis 5 Prozent.', 'start': 710.265, 'end': 735.742}, {'text': ' Und im MT-Banch ist es stark von der Kategorie abhängig. Wir sind im Vergleich zum zu der Lama 70-B. Wir sind von Leyen besser in Rollplay und in Reasoning, Reasoning mit 0,1. Aber trotzdem, das Modell ist Phoenix ist zehnmal kleiner. Das Lama 70-B. Und in den anderen Kategorien sind wir immer so einen Ticken drüber über dem gleich großen Bistrohl-Modell von Leyen.', 'start': 736.937, 'end': 763.797}, {'text': ' Was mich an meinem Brennen interessiert würde, wenn ich jetzt so ein Modell, nehme was erst mal, ja, prima auf Englisch, aber wenn man Multi-Linguar alt trainiert wurde, und ist durch so einer Nachtraining mit einer Sprache, so mal in dieser, bei Tests in dieser Sprache verbessere, nimmt die Performance bei anderen Sprachen dadurch ab oder wird das irgendwie kaum beeinflusst, haben sie das schon erste Erfahrung eintrücke sammeln können.', 'start': 763.848, 'end': 788.933}, {'text': ' Also es gibt dazu Tests von Google, die haben ja das MT-5 trainiert mit über 100 Sprachen. Denen ist aufgefallen, dass die mehr Sprachen des Modell letztendlich kann, dass du schlechtest die Performance in einer Sprache. Ich habe jetzt bisher nur das Modell getestet und auch noch keine anderen Sprachen, in der vom Swerbe anspannen des Thema für eine Master-Tesis für die Nachfolgen, dann sind auch die Tonnen von mir.', 'start': 789.616, 'end': 818.353}, {'text': ' absolut, absolut. Er ist ihm, glaube ich, auch wirklich noch ein etliche Test an, weil es einmal diese, und das hat mir ja auch schon in der einen anderen Sendung schon mal gesprochen, die ganzen Evelierungsfamburgs, die sind ja im Endeffekt, zum Bistel mit Vorsicht zu genießen, weil der Mendeffekt stander da ansetzen, benutzt werden. Und ich sehe im Freit auch noch mal, was sozusagen.', 'start': 819.07, 'end': 839.036}, {'text': ' Und man im Endeffekt natürlich auch das Risiko hat, dass diese Daten irgendwie zu trainieren, von Modellen, benützt wurden. Und damit dann die Kennzahlen besser werden. Und es ist natürlich auch so, jetzt in bestimmten Anwendungsfall, weiß man natürlich trotzdem nicht genau wie gut funktioniert das Modell.', 'start': 839.036, 'end': 854.821}, {'text': ' Das heißt, egal was für Modell, ob das für nichts Modell ist oder auch andere Modelle. Das ist eigentlich zwingend notwendig, wenn ich das jetzt irgendwo nützend möchte, in ganz konkreten Anwendungsfall, dass ich mir eigentlich mein eigenen Evolutionsdatenzats und Evolutionsmetrik für meinen Anwendungsfall aufbau und denen dann eigentlich nütze, um zu messen, wie gut ist das oder auch die anderen Modelle in dem Kontext.', 'start': 854.821, 'end': 877.073}, {'text': ' Da ist jetzt noch ganz schön Arbeit zu machen, auszusehen, wie reagiert das Modell in den täglichen Bromten, sage ich jetzt mal, wo sie noch mängel, wo es man nachlust. Ich erinnere nicht die letzte Version bleiben oder Herr Ulig. Da bin ich auch nicht da.', 'start': 877.073, 'end': 896.698}, {'text': ' Wenn wir es mal so die Gesamtprozess sehen, wenn wir schon gesehen, das waren eine relativ aufwendiges Zusammenzammeln, der Daten übersetzen. Also, es zeigt sicher, dass das mich so wahrscheinlich ein Großteil des Gesamtprozesses ausmacht, denn das trainieren, was natürlich automatisiert erfolgt, Rechnleistung und dann am Ende des überlügieren. Wo würden Sie denn, wenn Sie das mal so die Prozentualverteilung vom Aufwand betrachten, damit wir mal so als für die Zuhörerinnen und Zuhörer mal gefühlt kriegen, wo steckt wie viel Arbeit drin?', 'start': 896.954, 'end': 925.179}, {'text': ' Das ist eine gute Frage. Ich würde ja. Das war einfach viel Arbeit. Vielleicht 40, 40, 40, 20. Also das hat auch in der Übersetzung der Weile gedauert, das richtige Modell zu finden. So schauen, wie klickt man das hin, dass das effizient in großen Batches klappt und die Qualität auch stimmt.', 'start': 926.305, 'end': 947.927}, {'text': ' Im Training gab es verschiedene Modelle, das letztendlich über verschiedene Grafikarten gleichzeitig, also ein Distributed Training mit Deep Speed und ähnlichen hinzukriegen und dann auch eben Tests zu machen, mit Laura adaptern, mit Kyulora, was ja auf der Hochschule Infrastruktur, dann gut möglich war, da verschiedene Ansätze zu testen. Die Evaluation war für mich dann tatsächlich auch noch neu dieses Standardisierte evaluieren.', 'start': 948.626, 'end': 975.998}, {'text': ' Da steckt auch noch einiges, drin gerade eben dieses Mechendern und solche Geschichten, wo ich glaube, dass noch viel zu tun ist, wo man auch viel Experimentieren kann, um zu schauen, was steckt in den Modellen tatsächlich drin. Aber ich glaube, es wurde das Datentema und das Training, war mit dem auf Endexen. Wie viel GPUs brauchen denn jetzt um so ein Modell zu trinieren? Das ist die klassische Bewerlerantwortes kommt drauf an. Also in Q-Lora,', 'start': 976.681, 'end': 1004.957}, {'text': ' Training schafft man auf einer Grafikate mit 48 GB. Auch ein Lora-Training ist in der Größenordnung möglich für das komplette Feinschuning ist von Hagging Phase, letztendlich in Klasterm mit 8.100 ausgeschrieben, also 8.80 GB. Die ich dann getestet habe, also da ist dann schon mehr Rechen-Power nötig.', 'start': 1005.162, 'end': 1030.52}, {'text': ' wo dann aber auch die Frage ist, was es tatsächlich auch noch zu testen gilt, inwieweit in die Qualität vom voll trainierten Modell letztendlich abweicht von dem eines Kulora oder Laura adaptiert ist. Ich glaube, dann seh wir schon ganz gut erfahren, somit in Killing glaube ich. Genau, also ich habe auch noch für die Evaluation in der Hinterhand ein adapter für Laura und Kulora, die in der gleichen Art und Weise trainiert wurden, die mir auch noch mal testen könnten theoretisch.', 'start': 1031.698, 'end': 1059.002}, {'text': ' Ja, das ist glaube ich gar nicht schlecht, weil vielleicht sollte man den also sie zu neben der tun. Ja, nicht die YouTube hören hier zu. Aber die sollten wir vielleicht noch mit reingengen. Weil es ist tatsächlich so, dass wir das Modell dann natürlich bei uns in der Infrastruktur jetzt auch am Laufen haben. Und auch die Doktoranten, die jetzt im kommenten Geniellen promovieren oder in den Anliegen sind, in dem Gebieten dieses Modell jetzt immer mit Laufen lassen, um einfach zu sehen, wo steht man da an der Stelle?', 'start': 1060.06, 'end': 1087.466}, {'text': ' Das ist tatsächlich nicht ganz so einfach und ich weiß nicht, wie es irgendwie in der Ulleger, aber sie können ja so bitte was erzählen, zu dem eigentlich im Prozess, weil sie haben in den Schüge angefangen, sie sind ja auch nicht der klassische Informatiker und und Karilla schon seit 100 Jahren, sondern kommen wir eigentlich auch aus einer anderen Domäne, wenn ich es richtig im Kopf hab.', 'start': 1087.466, 'end': 1105.043}, {'text': ' wie wir haben sie denn lernen können. In diesen kleinen, in anderen Strichern, kleinen Nebenprojekt hat es ihm was gebracht. Was würden Sie so, wenn Sie Revue passieren lassen, was würden Sie dazu sagen? Herr, es ist völlig richtig. Ich komme eigentlich aus der BWL Richtung. Also ich habe BWL im Bachelor studiert. Mein erstes erstes Hello World ist, glaube ich, zwei Jahre her. Also ich habe vor zwei Jahren angefangen mit Programmieren von Null auf. Also wirklich hier Indexierung mit Null und solche Geschichten.', 'start': 1105.043, 'end': 1134.565}, {'text': ' Alles durch und das ging dann relativ schnell im Studiengang. Dadurch, dass wir sehr viel praktisch auch gemacht hatten und da viele Projekte war da schon der Einstieg ganz gut, sich eben mit den verschiedenen Themen zu beschäftigen. Und dann im zweiten Semester, wo dann das Thema richtig Fahrt aufgenommen hat, das natürlich im Menz viel auf einmal. Das ist zu bewältigen Gabe, aber es war auch sehr spannend, wirklich, dass eine wurde rausgebracht und hat zwei Tage später neu probiert.', 'start': 1135.401, 'end': 1164.462}, {'text': ' insgesamt ist der, was der Prozess sehr spannend natürlich auch stellenweise frustrieren wie überall aber nur so kann man seine Komfortzone ein bisschen pushen ja und das hat sehr viel Spaß gemacht das tolle ist ja dass die KI Community sehr viel teilt und Code und Erklärungen Tutorials bereitstellt in fastbahn vielzahl und Tiefe auch und mal dazu allem fast was findet was dann immer sehr', 'start': 1164.718, 'end': 1193.285}, {'text': ' sehr hilfreich, wenn man mal irgendwo gehangen ist, gerade auch im Trainingsprozess war da wirklich die, die Unterlagen, die von Hageinfas zur Verfügung gestellt wurden in dem Alleinment Handbook, waren wirklich Goldwert. Und ich habe jeden Fall sehr viel dazu lernen können.', 'start': 1193.712, 'end': 1209.445}, {'text': ' Ja, ich finde, das ist sehr beeindruckend, absolut. Ich muss an der Stelle auch noch mal einen Dank auch an den Super-Sanke mal. Das ist eine der Reptranden, die sie in dem Prozess gut begleitet hat. Und auch immer für alle Fragen offensichtlich, möchte ich nicht irgendwie hinten runterfallen, weil ich glaube ja auch einen Anteil an dem Thema.', 'start': 1210.333, 'end': 1230.879}, {'text': ' Wir können ja auf jeden Fall, ich bin gestern auf der Suche nach einem Screenshot, auch mal unseren Chat verlaufen, nochmal durchgegangen, und mir es da ist aufgefallen, wie viele wir uns tatsächlich über die verschiedenen Modelle und Versuche unterhalten haben, und uns dazu immer wieder ausgetauscht haben. Das ist guter Sand hat auf jeden Fall ein sehr großen Beitrag zu geleistet, auch immer wieder, wenn ich mal so kurz davor war zu sagen, nee, ich habe das keine Lust mehr,', 'start': 1231.015, 'end': 1253.183}, {'text': ' Und dann so heißt es, wo wir noch mal das. Und war auf jeden Fall eine große Hilfe, in dem ganzen Prozess, verstärkt auch in der Evaluation jetzt.', 'start': 1253.951, 'end': 1261.51}, {'text': ' Und jetzt muss ich aber auch noch mal eine Spefplizipreche an Sie, weil das waszenierende ist, man hat ja jeden Tag um neue Modelle raus. Und natürlich probieren wir auch in der Forschungsgruppe, wahnsinnig viel aus und mal dieses Modellen mal jenes. Aber was interessant ist, wenn natürlich ein Modell aufpuppt, das in den eigenen Reihen sozusagen entstanden ist, dann entsteht natürlich auch ein ganz andere', 'start': 1262.398, 'end': 1286.425}, {'text': ' in der Restensdrohne nicht jetzt mal, das mit der ganz anderen Art und Weise zu erfolgen. Und wenn ich mir jetzt die letzten drei, vier Tage anschau, wo es jetzt im Endeffekt darum gehen, dass auch die Personen, die nicht so involviert sind in Modell Training oder generell auf der Tiefe, in der plötzlich in der Reste haben, in der Evelierung mitzumachen und der Tiefe reinzugehen und sich jetzt Gedanken machen, was wir könnten Bomben aussehen.', 'start': 1287.432, 'end': 1309.121}, {'text': ' Ähnlich gesagt, die bromt sich jetzt alle gesehen, aber ich habe dann plötzlich Fragen gekriegt von den Dr. Andersen und Dr. Andersen. Wie ist denn das eigentlich, wenn wir jetzt da was böses eintragen, in die Applikationen, die wir zum Testen verwenden bei uns intern, was ja auch von uns entwickelt ist, also auch wieder aus der anderen Kuppe, nehmt es irgendwie einer mit, oder kriegt es mit, haben wir dann ein Problem, wenn die Accounts gesperrt oder ... Ja, alles was ich jetzt sagen kann, ich bin dagegen sie verwendet werden. So in der Art, ja. Wo ich jetzt eine例, das ist alles bei uns, und es ist ja, sind uns weg, wir wollen ja wissen,', 'start': 1309.121, 'end': 1339.07}, {'text': ' Was leichest, das Modell im Positive wie im Negativ. Man sieht einen Testen heißt ja nicht nur zu schauen, es ist jetzt besser als die anderen, sondern es ist anwendbar. Muss man das vielleicht gatorälen, was wir auch schon gesprochen haben, in anderen Podcasts Sendungen, müssen wir an bestimmten Stellen noch mal hinlagen, ist es sozusagen vertretbar, in der gewissen Weise ist bestimmte Branchen zu verwenden oder was auch immer.', 'start': 1339.07, 'end': 1361.8}, {'text': ' Und das war echt interessant, dass das jetzt so wichtig Vater aufgenommen hat, einfach nur, weil was ist, was, wo man die Person kennt, die es gemacht hat, weil es in dem direkten Kontext ist. Also das hat ein ganz anderen Puschnochmal. Also ich kann auch wirklich anderen Forschungsinstituten, oder Rohschulen, die die Möglichkeit haben, und da Interesse und Polizei haben, das nur andere Regen, die Studenten würden, wie ich weiß, wie wir es bei Ihnen hingegrecht haben, aber ich glaube, das sind sie eher selber nicht.', 'start': 1361.8, 'end': 1385.896}, {'text': ' Das sowas im Endeffekt parallel passiert, weil wenn man ja sich würdest man schätzen, wenn man Stunden einrichten, dann würde das sind schon mehrere Hundert Stunden, die da zusammenkommen, die sie da energie rein gesteckt haben. So gefühlt auch von, wenn man über die Zeitachse sieht und auch den Diskurs und so weiter kann man natürlich nicht einfach in der Veranstaltung packen oder erwarten, dass es jeder Student macht.', 'start': 1385.896, 'end': 1410.23}, {'text': ' Da reichen wahrscheinlich auch wieder die Rechenleistung, nicht die Paprika ist. Da muss ja auch nicht, aber ich finde es, das inspiriert. Und man sieht auf einmal Mensch. Der hat es auch gerade das angefangen zu lernen. Der hat es jetzt immer mal in der Kurz, zeig ja viele Stunden natürlich, aber am Ende sag ich das, weil sie eine Stunde wertfertigt hat sich mal. Und man sieht auf einmal Mensch, da sind Sachen machbar, wo ich vielleicht aber sonst nur gewartet habe, näher da kommt wieder um eine große', 'start': 1411.374, 'end': 1434.923}, {'text': ' In Zitu, was wieder neu ist, mit der herausgebracht hat, was er nicht kann, auch was beiitragen. Also ich finde es sehr, sehr beeindruckend inspirieren. Ja, da was auch sehr schön die Freiheit zu haben, sich bei uns an der Hochschule auf den Server einzulogeln und sagen, okay, das lag ich das Modell mal runter und probiert es einfach mal und testet was Neues und da hat er erst selber schon noch eine größere Kapazität, als mein Gaming-Richt nach zu Hause.', 'start': 1434.923, 'end': 1458.916}, {'text': ' Und da war einfach die Freiheit da, verschiedene Sachen, einfach mal zu probieren und es ging dann auch relativ schnell. Aber trotzdem, es muss ich nochmal mal sagen. Ich meine, es fällt mir jetzt schwer zu glauben, dass hier jetzt da sich hingesetzt haben und gesagt haben, Mensch, oh, mir ist gerade langweilig am Wochenende, was mache ich denn mal, ich brauche jetzt ein Modell, was besser ist als alles andere, weil das ihr vortreiniert plan oder wollten sie einfach erst mal', 'start': 1459.07, 'end': 1482.381}, {'text': ' oder jetzt trainieren und gucken und dann dann auf einmal festgestellt so war, das funktioniert. Das ist gar nicht schlecht. Ich meine, was war Ihre Intensionen ursprünglich? Ich glaube, das hat sich so langsam aufgebaut. Das hat damit angefangen, einfach zum einen, dieses, ich möchte dieses Hacking Phase Ökosystem besser kennenlernen, verstehen, wie das funktioniert. Ich habe dann auch mal nach der Empfehlung von Herrn Schacht ein Video vom André Kapati.', 'start': 1482.381, 'end': 1506.169}, {'text': ' Wir angeschaut, wo das eigentlich ein GPT-Modell nachbaut, wo ich dann drei Stunden da Vorsatz um den Code letztendlich zu verstehen, und zum anderen hat es mir einfach Spaß gemacht, Modelle zu trainieren. Und das ist dann so langsam in die Mastertesis übergegangen, wo ich dann auf der Suche war, nach Deutschen Modellen für mein Projekt für mein Thema, wo dann so ein bisschen Frustration aufkam, einfach die Auswahl, an Deutschen Modellen, zu dem Zeitpunkt noch relativ eingeschränkt war.', 'start': 1506.783, 'end': 1534.138}, {'text': ' und so hat sich das dann langsam hochgeschaukelt, bis ich dann dazu gekommen bin, okay, jetzt schauen wir mal, was wir mit diesem speziellen Ansatz, direct, direct, preference, optimisation. Jetzt mit mich ja mal auch mal, ich kann mir vorstellen, dass es jetzt auch die die zuhörinnen und zuhörer auch interessiert, jemand, der jetzt so ein Wochenende arbeitet und mit der Stützung, aber so ein Modell gebaut hat, was machen Sie denn jetzt in ihrer Master-Tesis?', 'start': 1535.299, 'end': 1562.176}, {'text': ' In meiner Master Tases kümmere ich mich, um in Verrennsoptimierung. Das heißt, ich beschäftige mich mit Quantisierung, Pluning, auch Spekulativ, die Koding und solchen Geschichten, um letztendlich zu testen, wie sich diese Methoten auf Modelle in der deutschen Sprache auswirken. Das sind inspiriert aus, denen Erfahrungen jetzt, die sie dabei gesammelt haben. Genau, also ich teste auch unter anderem, dass sie auf Modell mit.', 'start': 1562.244, 'end': 1588.729}, {'text': ' Und es schneidet in den aktuellen Test vergleichsweise gut ab, dafür, dass es hier an reinen Englisch tringet das Modell ist. Bringt es auf deutsch schon eine beachtliche Leistung im Vergleich zu dem, was jetzt die Lama Modelle bringen. Ja, und ich war einfach begeistert, von dem, was das Team dahinter, das ist ja von Hageingface vom Hageingface Team mit geschrieben, was sie auf die Beine gestellt haben und was sie auch ein Material zur Verfügung gestellt haben, eben die Datensätze,', 'start': 1589.684, 'end': 1616.425}, {'text': ' den UltraChat Datensatz selbst, endlich von mir auch verwendet wurde. Ja, und das war dann definitiv inspirierend.', 'start': 1617.005, 'end': 1624.104}, {'text': ' und mit Grundlage dafür, dass ich sage, ich probiere das mal für Deutsch aus. Wenn ohne Sie jetzt da unter Druck setzen zu wollen, aber da dem, was Sie hier vorgelagt haben, legt haben, bin ich jetzt sehr gespannt, was da den rauskommt und vielleicht reden wir da ja in einem halben Jahr wieder mit ihnen im Podcast darüber. Ich hoffe, ich hoffe, nicht, dass es ein halbes Jahr hat, aber das gibt ja noch so ein Mix-Shirt-Off-Expert-Modelle und könnt ihr noch so einem Jahr, wo ich schon jetzt gedacht habe, das ist ein halbes Jahr läuft, aber ich weiß nicht. Ja, das ist ein halben Jahr. Ja, das ist ein halben Jahr. Ja, das ist ein halben Jahr. Ja, das ist ein halben Jahr. Ja, das ist ein halben Jahr.', 'start': 1624.821, 'end': 1653.712}, {'text': ' Oh, das ist läuft schon nur. Okay. Also es gibt schon noch paar Sachen, die man angehen kann. Die Frage ist noch, wie wir die Zeilheit erholigen. Also, weil Sie es gerade angesprochen haben, so diesen Mann könnte es auch mal mit kleineren Modellen testen. Ich habe es tatsächlich mit dem Tiny-Lama getestet. Das hier noch, ich glaube vor zwei oder drei Wochen auskommen, ist an einem belierten Parametermodell trainiert mit drei Billionentogen. Wenn ich es noch richtig weiß, die Ergebnisse sind teilweise lustig.', 'start': 1654.189, 'end': 1681.544}, {'text': ' worauf dieses Modell aber sehr stark anfällig war, war das Thema, dass es sich einfach repetitiv wieder holt und ihren Promt nicht beendet bekommt. Das ist fast was auch beim Phoenix Modell, immer wieder zu beobachten ist, aber das schlägt es eine Milliardeparamide Modell noch stärker drauf an.', 'start': 1681.817, 'end': 1697.602}, {'text': ' Also ich finde inspirierend, was mir ehrlich gesagt an dem ganzen Spiechen so gut gefällt, ist natürlich einerseits, es ist natürlich von den Kennzahlen her, eine sehr gute Performance einfach hinbekommen und es sein dann nochmal anstichelt. Aber was natürlich schon spannend ist, das ist halt wirklich zeigt, dass es im KI umfeld stand heute eigentlich gar nicht mehr so auf das ganz extrem große Budget und', 'start': 1697.739, 'end': 1723.797}, {'text': ' Hunderte von Manpower ankommt, sondern kreativität aus dauer Interesse ist eigentlich in den hohe Faktor, in meinen Oben.', 'start': 1723.797, 'end': 1738.507}, {'text': ' Das ist ein wunderbares Schlusswort. Ich wollte jetzt sagen, vor allem nicht total, wo liegt das, sie sich Zeit genommen haben, vor allem nicht total, dass sie mit uns zu sagen, ihr Wissen geteilt haben, natürlich auch, dass sie aus den Schwingern, aus diese Ideen sozusagen weitergetrieben haben und die das Baby dazu archaif, wenn wir in die Schone kurz mit reinpackten, sofern archaif ist, dann bis dahin freigegeben hat. Ich trage das nach. Ja, wir nicht glaubmass nach, genau, das geht ja auch.', 'start': 1739.428, 'end': 1766.988}, {'text': ' von daher noch mal vielen vielen vielen Dank. Und wir freuen uns auf, jetzt ist die Frage, wie heißt fönigst, wenn es ein Mix-Shop expert Modell wird? Ja, habe ich schon überlegt. Ich bin mir noch nicht ganz sicher. Wie lasst uns überraschen? Also vielen Dank, dass ich dabei waren und... Sehr gut. Ja, viel Erfolg bei der Arbeit und der Master dieses. Vielen Dank. Vielen Dank, der Rudi. Wann ist es gut? Vielen Dank an die Zuhörger auch. Und wir freuen uns, wenn sie nicht sowohl römiert hat, wieder mit dabei sind.', 'start': 1766.988, 'end': 1795.316}, {'text': ' Das war eine weitere Folge des Knowledge Science Podcasts. Vergessen Sie nicht, nächste Woche wieder dabei zu sein. Vielen Dank fürs Zuhören.', 'start': 1799.753, 'end': 1809.053}]

    # Insert Podcast
    if podcast_detail:
        podcast_combo = podcast['title'] + ' ' + podcast['description']
        podcast_embedding = [embed_text(podcast_combo, model)]
        podcast_obj = {
                'id': [podcast['id']],
                'title': [podcast['title']],
                'description':  [podcast['description']],
                'author': [podcast['author']],
                'image': [podcast['image']],
                'embedding': podcast_embedding
            }
        df_podcast = pd.DataFrame.from_dict(podcast_obj, orient='index')
        df_podcast = df_podcast.transpose()

        insert("podcast", df_podcast)

    episode_combo = episode['title'] + ' ' + episode['description']
    episode_embedding = [embed_text(episode_combo, model)]

    # Insert Episode
    episode_obj = {
            'id': [episode['id']],
            'title': [episode['title']],
            'description': [episode['description']],
            'datePublishedPretty': [episode['datePublishedPretty']],
            'image': [episode['image']],
            'enclosureUrl': [episode['enclosureUrl']],
            'podcast_id': [podcast_detail['feed']['id']],
            'embedding': episode_embedding
        }
    df_episode = pd.DataFrame.from_dict(episode_obj, orient='index')
    df_episode = df_episode.transpose()

    insert("episode", df_episode)

    # Insert Segment
    text_vector = []
    start_vector = []
    end_vector = []
    segment_embedding = []

    for i in range(len(transcript_parsed)):
        try:
            text_vector.append(transcript_parsed[i]["text"] if i < len(transcript_parsed) - 1 else '')
            start_vector.append(transcript_parsed[i]["start"] if i < len(transcript_parsed) - 1 else -1)
            end_vector.append(transcript_parsed[i]["end"] if i < len(transcript_parsed) - 1 else -1)

            episode_embedding = embed_text(transcript_parsed[i]["text"], model)
            segment_embedding.append(episode_embedding)
        except IndexError:
            print('Index error ' + str(i))

    # Insert transcription segments
    segments = {
        'episode_id': [episode['id'] for obj in range(len(transcript_parsed))],
        'text': text_vector,
        'start': start_vector,
        'end': end_vector,
        'speaker': ['unknown' for obj in range(len(transcript_parsed))],
        'embedding': segment_embedding
    }

    df_segments = pd.DataFrame.from_dict(segments, orient='index')
    df_segments = df_segments.transpose()

    insert("segment", df_segments)

    return transcript_parsed


# Get Podcast-List
@app.get("/v1/vdb/podcasts")
def get_vdb_podcast_list():
    return get_podcast_list()


# Get Episode-List By Podcast Id
@app.get("/v1/vdb/episode/{podcast_id}")
def get_vdb_episode_list_by_podcast_id(podcast_id: str):
    return get_episode_list_by_podcast_id(podcast_id)


# Get Segment-List By Episode Id
@app.get("/v1/vdb/episode/{episode_id}/segments")
def get_vdb_segment_list_by_episode_id(episode_id: str):
    segments = get_segment_list_by_episode_id(collection_name='segment', max_dimension=max_dimension, episode_id=episode_id)
    filtered_segments = [segment for segment in segments if segment['text'] != '']
    return filtered_segments
